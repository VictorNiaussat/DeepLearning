{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":4670,"status":"ok","timestamp":1671563810382,"user":{"displayName":"victor cilleros","userId":"04822214906677236965"},"user_tz":-60},"id":"Gktm9P8aMWPU"},"outputs":[],"source":["import os\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","import cv2\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import random\n","from zipfile import ZipFile\n","from google.colab.patches import cv2_imshow\n","import tensorflow as tf\n","from tensorflow.keras import models,optimizers,layers\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","from tensorflow.keras.callbacks import TensorBoard\n","\n","from tensorflow.python.keras import layers \n","from tensorflow.python.keras import models\n","from tensorflow.python.keras import optimizers\n","from keras.callbacks import TensorBoard\n","from keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.utils import img_to_array\n","from tensorflow.keras.utils import load_img\n","from tensorflow.keras.utils import plot_model\n","import pydot_ng as pydot\n","from sklearn.model_selection  import train_test_split\n","import math"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17958,"status":"ok","timestamp":1671563832125,"user":{"displayName":"victor cilleros","userId":"04822214906677236965"},"user_tz":-60},"id":"j_o9t7_qMYmT","outputId":"06752498-8736-4a5c-83e0-d87881b926ca"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Monter votre Drive afin de pouvoir y'acceder \n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"qkbdigJNMFfk"},"source":["# Facial emotion recognition\n","The aim of this lab (TP) is to is to recognize facial emotions from a video using deep learning\n","approach. The system can take pictures or video form webcam as input. It detects all faces in\n","each frame, and then classifies the emotions as belonging to one of the 7 emotion categories:\n","Angry, Disgusted, Neutral, Sad, Happy, Surprised and Fear.\n","Facial landmarks are used to localize and represent salient regions of the face, such as: eyes,\n","eyebrows nose, mouth jawline. They have been successfully applied to face alignment, head\n","pose estimation, face swapping, blink detection and much more."]},{"cell_type":"markdown","metadata":{"id":"aJyuLdj5MJAL"},"source":["We propose here to develop an approach based on famous facial landmarks (68 facial landmarks can be detected with dlib library), which and use the Eucledian distance between them as 68x68 array and use them as feature to learn a network"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":377,"status":"ok","timestamp":1671563836083,"user":{"displayName":"victor cilleros","userId":"04822214906677236965"},"user_tz":-60},"id":"WOdLrkKdMCD1"},"outputs":[],"source":["def euclidean(a, b):\n","    dist = math.sqrt(math.pow((b[0] - a[0]), 2) + math.pow((b[1] - a[1]), 2))\n","    return dist \n","\n","def euclidean_all(a):  # calculates distances between all 68 elements\n","\tdistances = \"\"\n","\tfor i in range(0, len(a)):\n","\t\tfor j in range(0, len(a)):\n","\t\t\tdist = euclidean(a[i], a[j])\n","\t\t\tdist = \"%.2f\" % dist;\n","\t\t\tdistances = distances + \" \" + str(dist)\n","\treturn distances"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1671563836778,"user":{"displayName":"victor cilleros","userId":"04822214906677236965"},"user_tz":-60},"id":"9q4KGUyz0bOw"},"outputs":[],"source":["\tdef euclidean_all_numpy(a):  # calculates distances between all 68 elements\n","\t\tdistances = []\n","\t\tfor i in range(0, len(a)):\n","\t\t\tfor j in range(0, len(a)):\n","\t\t\t\tdist = euclidean(a[i], a[j])\n","\t\t\t\tdistances.append(dist)\n","\t\treturn np.array(distances)"]},{"cell_type":"markdown","metadata":{"id":"qd_VSWCJNXDX"},"source":["To train this network, you can use the dataset (train_test_landmarks.zip extracted from a\n","mixture of CK+ and JAFFE datasets) already prepared as feature vector of distances with the\n","format:(741, 4624) for train and (365, 4624) for test. After the training, save you best model.\n","https://drive.google.com/file/d/1teun1oa2nolShwfpksV-nOdeWCX484ax/view?usp=sharing"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":724,"status":"ok","timestamp":1671563843327,"user":{"displayName":"victor cilleros","userId":"04822214906677236965"},"user_tz":-60},"id":"SLfoFLXVMQH7","outputId":"5d1d8b2d-07f2-4552-c203-96804610937f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense (Dense)                (None, 512)               2368000   \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 512)               262656    \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 512)               262656    \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 512)               262656    \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 512)               262656    \n","_________________________________________________________________\n","dense_5 (Dense)              (None, 7)                 3591      \n","=================================================================\n","Total params: 3,422,215\n","Trainable params: 3,422,215\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["model = models.Sequential()                                                              \n","\n","model.add(layers.Dense(512, activation='relu',input_shape=(4624,)))\n","model.add(layers.Dense(512, activation='relu'))\n","model.add(layers.Dense(512, activation='relu'))\n","model.add(layers.Dense(512, activation='relu'))\n","model.add(layers.Dense(512, activation='relu'))\n","\n","model.add(layers.Dense(7, activation='softmax')) \n","\n","model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['acc'])\n","model.summary()"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3539,"status":"ok","timestamp":1671563847220,"user":{"displayName":"victor cilleros","userId":"04822214906677236965"},"user_tz":-60},"id":"UZAa4adYQ0gK","outputId":"8448de69-f3b8-43bf-c2b3-a42fdfe415cb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Done\n"]}],"source":["from zipfile import ZipFile\n","file_name = \"/content/drive/MyDrive/DeepLearning_SDI/facial_ressources/Ressources.zip\"\n","\n","with ZipFile(file_name, 'r') as zip:\n","  zip.extractall()\n","  print('Done')"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":770,"status":"ok","timestamp":1671563847987,"user":{"displayName":"victor cilleros","userId":"04822214906677236965"},"user_tz":-60},"id":"XNhKJQHMOKls","outputId":"dfde8b49-2f58-46bc-afa1-e20e1efd11a8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Done\n","Done\n"]}],"source":["from zipfile import ZipFile\n","file_name = \"./Ressources/train_test_landmarks.zip\"\n","\n","with ZipFile(file_name, 'r') as zip:\n","  zip.extractall()\n","  print('Done')\n","\n","file_name = \"/content/Ressources/train_test_landmarks.zip\"\n","\n","with ZipFile(file_name, 'r') as zip:\n","  zip.extractall()\n","  print('Done')"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1671563847988,"user":{"displayName":"victor cilleros","userId":"04822214906677236965"},"user_tz":-60},"id":"fw_PvbbSRsei"},"outputs":[],"source":["X_test= np.load('./x_test.npy')\n","X_train= np.load('./x_train.npy')\n","y_test = np.load('./y_test.npy')\n","y_train = np.load('./y_train.npy')"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1671563847988,"user":{"displayName":"victor cilleros","userId":"04822214906677236965"},"user_tz":-60},"id":"4a-pheggSZXp","outputId":"b95d32f1-d2c3-4c66-daad-2fe588e3fe58"},"outputs":[{"output_type":"stream","name":"stdout","text":["(365, 4624)\n","(741, 4624)\n","(365, 7)\n","(741, 7)\n"]}],"source":["print(X_test.shape)\n","print(X_train.shape)\n","print(y_test.shape)\n","print(y_train.shape)"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1671563849041,"user":{"displayName":"victor cilleros","userId":"04822214906677236965"},"user_tz":-60},"id":"f1jVLzmgSVYx"},"outputs":[],"source":["batch_size = 32\n","NAME = \"TENSORBOARD_EMOTION\"\n","tensorboard = TensorBoard(log_dir=\"./logs/{}\".format(NAME))"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":364},"executionInfo":{"elapsed":1362,"status":"error","timestamp":1671563851207,"user":{"displayName":"victor cilleros","userId":"04822214906677236965"},"user_tz":-60},"id":"QdxDBU07S3KB","outputId":"6b7dd28f-8aa2-457e-e968-4de9b9b8c19c"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Launching TensorBoard..."]},"metadata":{}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-245ad34674e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlogs_base_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./logs\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs_base_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tensorboard'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'--logdir {logs_base_dir}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2312\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2313\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2314\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2315\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorboard/notebook.py\u001b[0m in \u001b[0;36m_start_magic\u001b[0;34m(line)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_start_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;34m\"\"\"Implementation of the `%tensorboard` line magic.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorboard/notebook.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(args_string)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0mparsed_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshlex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m     \u001b[0mstart_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStartLaunched\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorboard/manager.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(arguments, timeout)\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0mend_time_seconds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_time_seconds\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_seconds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mend_time_seconds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoll_interval_seconds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         \u001b[0msubprocess_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msubprocess_result\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["%load_ext tensorboard\n","import datetime\n","logs_base_dir = \"./logs\"\n","os.makedirs(logs_base_dir, exist_ok=True)\n","%tensorboard --logdir {logs_base_dir}"]},{"cell_type":"markdown","metadata":{"id":"EV1Gxr8Xu2jB"},"source":["history_Dense= model.fit(X_train,y_train,epochs=32,validation_data=(X_test,y_test),batch_size=batch_size,callbacks=[tensorboard])"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":246,"status":"ok","timestamp":1671563854152,"user":{"displayName":"victor cilleros","userId":"04822214906677236965"},"user_tz":-60},"id":"bH6OhtFbMraZ"},"outputs":[],"source":["checkpoint_filepath = './tmp/checkpoint'\n","model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_filepath,\n","    save_weights_only=True,\n","    monitor='val_accuracy',\n","    mode='max',\n","    save_best_only=True)"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":104962,"status":"ok","timestamp":1671563960018,"user":{"displayName":"victor cilleros","userId":"04822214906677236965"},"user_tz":-60},"id":"pjOGX7Iju7JQ","outputId":"ec492616-9fe6-438a-ab0f-c07f14ef0525"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","24/24 [==============================] - 2s 48ms/step - loss: 1.7056 - acc: 0.5074 - val_loss: 1.3295 - val_acc: 0.6027\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2/100\n","24/24 [==============================] - 1s 34ms/step - loss: 1.1768 - acc: 0.6302 - val_loss: 1.2051 - val_acc: 0.6658\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3/100\n","24/24 [==============================] - 1s 37ms/step - loss: 0.9823 - acc: 0.6896 - val_loss: 0.7780 - val_acc: 0.7589\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4/100\n","24/24 [==============================] - 1s 37ms/step - loss: 0.8619 - acc: 0.7152 - val_loss: 0.8157 - val_acc: 0.7096\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5/100\n","24/24 [==============================] - 1s 36ms/step - loss: 0.8118 - acc: 0.7314 - val_loss: 0.7433 - val_acc: 0.7562\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6/100\n","24/24 [==============================] - 1s 37ms/step - loss: 0.7576 - acc: 0.7436 - val_loss: 0.7930 - val_acc: 0.7397\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7/100\n","24/24 [==============================] - 1s 35ms/step - loss: 0.7137 - acc: 0.7598 - val_loss: 0.8860 - val_acc: 0.6822\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 8/100\n","24/24 [==============================] - 1s 36ms/step - loss: 0.6475 - acc: 0.7638 - val_loss: 1.0537 - val_acc: 0.5836\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9/100\n","24/24 [==============================] - 1s 44ms/step - loss: 0.5939 - acc: 0.7962 - val_loss: 0.9974 - val_acc: 0.6055\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 10/100\n","24/24 [==============================] - 2s 67ms/step - loss: 0.5706 - acc: 0.8057 - val_loss: 0.7760 - val_acc: 0.7616\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 11/100\n","24/24 [==============================] - 2s 80ms/step - loss: 0.6638 - acc: 0.7638 - val_loss: 0.5886 - val_acc: 0.8110\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 12/100\n","24/24 [==============================] - 2s 73ms/step - loss: 0.6200 - acc: 0.8016 - val_loss: 0.6682 - val_acc: 0.7753\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 13/100\n","24/24 [==============================] - 2s 66ms/step - loss: 0.6014 - acc: 0.7922 - val_loss: 0.5774 - val_acc: 0.8219\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 14/100\n","24/24 [==============================] - 2s 66ms/step - loss: 0.5539 - acc: 0.7935 - val_loss: 1.0053 - val_acc: 0.6740\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 15/100\n","24/24 [==============================] - 2s 69ms/step - loss: 0.7215 - acc: 0.7517 - val_loss: 0.7391 - val_acc: 0.7616\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 16/100\n","24/24 [==============================] - 2s 70ms/step - loss: 0.6573 - acc: 0.7746 - val_loss: 0.6432 - val_acc: 0.7781\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 17/100\n","24/24 [==============================] - 2s 71ms/step - loss: 0.5380 - acc: 0.8084 - val_loss: 0.6280 - val_acc: 0.7918\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 18/100\n","24/24 [==============================] - 2s 76ms/step - loss: 0.5104 - acc: 0.8003 - val_loss: 0.8314 - val_acc: 0.6986\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 19/100\n","24/24 [==============================] - 2s 72ms/step - loss: 0.5216 - acc: 0.8097 - val_loss: 0.6682 - val_acc: 0.7589\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 20/100\n","24/24 [==============================] - 2s 77ms/step - loss: 0.4267 - acc: 0.8367 - val_loss: 0.7401 - val_acc: 0.7479\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 21/100\n","24/24 [==============================] - 1s 61ms/step - loss: 0.4159 - acc: 0.8529 - val_loss: 0.8226 - val_acc: 0.6959\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 22/100\n","24/24 [==============================] - 1s 38ms/step - loss: 0.4422 - acc: 0.8394 - val_loss: 1.2063 - val_acc: 0.6164\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 23/100\n","24/24 [==============================] - 1s 40ms/step - loss: 0.6706 - acc: 0.7760 - val_loss: 0.7389 - val_acc: 0.7616\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 24/100\n","24/24 [==============================] - 1s 38ms/step - loss: 0.5933 - acc: 0.7746 - val_loss: 0.6631 - val_acc: 0.7644\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 25/100\n","24/24 [==============================] - 1s 36ms/step - loss: 0.5372 - acc: 0.8138 - val_loss: 0.5807 - val_acc: 0.8137\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 26/100\n","24/24 [==============================] - 2s 73ms/step - loss: 0.4787 - acc: 0.8408 - val_loss: 0.8555 - val_acc: 0.7096\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 27/100\n","24/24 [==============================] - 2s 76ms/step - loss: 0.4961 - acc: 0.8313 - val_loss: 0.8133 - val_acc: 0.7205\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 28/100\n","24/24 [==============================] - 2s 81ms/step - loss: 0.4201 - acc: 0.8543 - val_loss: 0.7608 - val_acc: 0.7781\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 29/100\n","24/24 [==============================] - 2s 100ms/step - loss: 0.3834 - acc: 0.8637 - val_loss: 0.5454 - val_acc: 0.8521\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 30/100\n","24/24 [==============================] - 2s 73ms/step - loss: 0.3981 - acc: 0.8610 - val_loss: 1.0390 - val_acc: 0.6712\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 31/100\n","24/24 [==============================] - 1s 36ms/step - loss: 0.4386 - acc: 0.8313 - val_loss: 0.9364 - val_acc: 0.6247\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 32/100\n","24/24 [==============================] - 1s 38ms/step - loss: 0.4568 - acc: 0.8327 - val_loss: 0.5955 - val_acc: 0.8192\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 33/100\n","24/24 [==============================] - 1s 36ms/step - loss: 0.3795 - acc: 0.8691 - val_loss: 0.6786 - val_acc: 0.7945\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 34/100\n","24/24 [==============================] - 1s 35ms/step - loss: 0.4442 - acc: 0.8394 - val_loss: 0.7039 - val_acc: 0.7699\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 35/100\n","24/24 [==============================] - 1s 36ms/step - loss: 0.3706 - acc: 0.8610 - val_loss: 0.8173 - val_acc: 0.7233\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 36/100\n","24/24 [==============================] - 1s 35ms/step - loss: 0.3357 - acc: 0.8772 - val_loss: 0.9474 - val_acc: 0.7123\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 37/100\n","24/24 [==============================] - 1s 37ms/step - loss: 0.4307 - acc: 0.8502 - val_loss: 0.8326 - val_acc: 0.7425\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 38/100\n","24/24 [==============================] - 1s 37ms/step - loss: 0.3008 - acc: 0.8988 - val_loss: 0.5406 - val_acc: 0.8438\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 39/100\n","24/24 [==============================] - 1s 36ms/step - loss: 0.2474 - acc: 0.9055 - val_loss: 0.5602 - val_acc: 0.8521\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 40/100\n","24/24 [==============================] - 1s 36ms/step - loss: 0.3909 - acc: 0.8718 - val_loss: 0.4976 - val_acc: 0.8438\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 41/100\n","24/24 [==============================] - 1s 34ms/step - loss: 0.2995 - acc: 0.8974 - val_loss: 1.1071 - val_acc: 0.6575\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 42/100\n","24/24 [==============================] - 1s 37ms/step - loss: 0.3040 - acc: 0.9001 - val_loss: 0.6123 - val_acc: 0.8356\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 43/100\n","24/24 [==============================] - 1s 35ms/step - loss: 0.3475 - acc: 0.8839 - val_loss: 0.8097 - val_acc: 0.7534\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 44/100\n","24/24 [==============================] - 1s 38ms/step - loss: 0.2690 - acc: 0.9109 - val_loss: 0.7408 - val_acc: 0.7918\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 45/100\n","24/24 [==============================] - 1s 37ms/step - loss: 0.3129 - acc: 0.8934 - val_loss: 0.5550 - val_acc: 0.8247\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 46/100\n","24/24 [==============================] - 1s 36ms/step - loss: 0.2611 - acc: 0.9123 - val_loss: 0.8731 - val_acc: 0.7534\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 47/100\n","24/24 [==============================] - 1s 35ms/step - loss: 0.3777 - acc: 0.8650 - val_loss: 0.5857 - val_acc: 0.8356\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 48/100\n","24/24 [==============================] - 1s 37ms/step - loss: 0.3612 - acc: 0.8785 - val_loss: 0.7161 - val_acc: 0.7726\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 49/100\n","24/24 [==============================] - 1s 36ms/step - loss: 0.2956 - acc: 0.8988 - val_loss: 0.5726 - val_acc: 0.8521\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 50/100\n","24/24 [==============================] - 1s 37ms/step - loss: 0.2831 - acc: 0.9001 - val_loss: 0.6825 - val_acc: 0.8055\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 51/100\n","24/24 [==============================] - 1s 37ms/step - loss: 0.2483 - acc: 0.9096 - val_loss: 0.9405 - val_acc: 0.7397\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 52/100\n","24/24 [==============================] - 1s 37ms/step - loss: 0.2323 - acc: 0.9150 - val_loss: 0.7674 - val_acc: 0.7781\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 53/100\n","24/24 [==============================] - 1s 37ms/step - loss: 0.3107 - acc: 0.8947 - val_loss: 0.8300 - val_acc: 0.7123\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 54/100\n","24/24 [==============================] - 1s 36ms/step - loss: 0.2821 - acc: 0.9069 - val_loss: 0.6295 - val_acc: 0.8521\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 55/100\n","24/24 [==============================] - 1s 35ms/step - loss: 0.2467 - acc: 0.9150 - val_loss: 0.7666 - val_acc: 0.7973\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 56/100\n","24/24 [==============================] - 1s 35ms/step - loss: 0.2358 - acc: 0.9150 - val_loss: 0.8380 - val_acc: 0.7726\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 57/100\n","24/24 [==============================] - 1s 37ms/step - loss: 0.2836 - acc: 0.9042 - val_loss: 0.8602 - val_acc: 0.7041\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 58/100\n","24/24 [==============================] - 1s 36ms/step - loss: 0.3099 - acc: 0.8988 - val_loss: 0.5204 - val_acc: 0.8438\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 59/100\n","24/24 [==============================] - 1s 37ms/step - loss: 0.2682 - acc: 0.9055 - val_loss: 0.7721 - val_acc: 0.7918\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 60/100\n","24/24 [==============================] - 1s 37ms/step - loss: 0.2658 - acc: 0.9015 - val_loss: 0.9823 - val_acc: 0.7397\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 61/100\n","24/24 [==============================] - 1s 36ms/step - loss: 0.2429 - acc: 0.9177 - val_loss: 0.5485 - val_acc: 0.8630\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 62/100\n","24/24 [==============================] - 1s 37ms/step - loss: 0.2769 - acc: 0.9028 - val_loss: 0.6167 - val_acc: 0.8301\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 63/100\n","24/24 [==============================] - 1s 35ms/step - loss: 0.3860 - acc: 0.8772 - val_loss: 1.1440 - val_acc: 0.6959\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 64/100\n","24/24 [==============================] - 1s 35ms/step - loss: 0.3726 - acc: 0.8812 - val_loss: 0.4844 - val_acc: 0.8411\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 65/100\n","24/24 [==============================] - 1s 39ms/step - loss: 0.3572 - acc: 0.8893 - val_loss: 0.8982 - val_acc: 0.7452\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 66/100\n","24/24 [==============================] - 1s 36ms/step - loss: 0.2253 - acc: 0.9217 - val_loss: 0.5359 - val_acc: 0.8603\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 67/100\n","24/24 [==============================] - 1s 36ms/step - loss: 0.1855 - acc: 0.9379 - val_loss: 0.7027 - val_acc: 0.8000\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 68/100\n","24/24 [==============================] - 1s 35ms/step - loss: 0.2911 - acc: 0.9136 - val_loss: 0.8310 - val_acc: 0.7863\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 69/100\n","24/24 [==============================] - 1s 37ms/step - loss: 0.2447 - acc: 0.9123 - val_loss: 0.8455 - val_acc: 0.7671\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 70/100\n","24/24 [==============================] - 1s 36ms/step - loss: 0.1893 - acc: 0.9285 - val_loss: 1.7829 - val_acc: 0.5644\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 71/100\n","24/24 [==============================] - 1s 37ms/step - loss: 0.3145 - acc: 0.8934 - val_loss: 0.6270 - val_acc: 0.8219\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 72/100\n","24/24 [==============================] - 1s 35ms/step - loss: 0.2312 - acc: 0.9258 - val_loss: 0.6469 - val_acc: 0.8082\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 73/100\n","24/24 [==============================] - 1s 37ms/step - loss: 0.1381 - acc: 0.9501 - val_loss: 1.0505 - val_acc: 0.7014\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 74/100\n","24/24 [==============================] - 1s 36ms/step - loss: 0.2586 - acc: 0.9069 - val_loss: 0.6625 - val_acc: 0.8137\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 75/100\n","24/24 [==============================] - 1s 36ms/step - loss: 0.2177 - acc: 0.9285 - val_loss: 0.6479 - val_acc: 0.8110\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 76/100\n","24/24 [==============================] - 1s 37ms/step - loss: 0.2203 - acc: 0.9204 - val_loss: 0.8769 - val_acc: 0.7836\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 77/100\n","24/24 [==============================] - 1s 39ms/step - loss: 0.2376 - acc: 0.9163 - val_loss: 0.9964 - val_acc: 0.7014\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 78/100\n","24/24 [==============================] - 1s 38ms/step - loss: 0.2328 - acc: 0.9177 - val_loss: 0.5505 - val_acc: 0.8548\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 79/100\n","24/24 [==============================] - 1s 38ms/step - loss: 0.1414 - acc: 0.9622 - val_loss: 0.9445 - val_acc: 0.7699\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 80/100\n","24/24 [==============================] - 1s 38ms/step - loss: 0.3196 - acc: 0.9042 - val_loss: 0.9507 - val_acc: 0.7616\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 81/100\n","24/24 [==============================] - 1s 35ms/step - loss: 0.2979 - acc: 0.9109 - val_loss: 0.8389 - val_acc: 0.7479\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 82/100\n","24/24 [==============================] - 1s 35ms/step - loss: 0.1673 - acc: 0.9447 - val_loss: 1.0893 - val_acc: 0.7726\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 83/100\n","24/24 [==============================] - 1s 34ms/step - loss: 0.2001 - acc: 0.9244 - val_loss: 0.7600 - val_acc: 0.8192\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 84/100\n","24/24 [==============================] - 1s 37ms/step - loss: 0.1809 - acc: 0.9217 - val_loss: 0.7456 - val_acc: 0.8356\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 85/100\n","24/24 [==============================] - 1s 37ms/step - loss: 0.3586 - acc: 0.8812 - val_loss: 0.6624 - val_acc: 0.8110\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 86/100\n","24/24 [==============================] - 1s 37ms/step - loss: 0.2375 - acc: 0.9096 - val_loss: 0.9584 - val_acc: 0.7808\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 87/100\n","24/24 [==============================] - 1s 36ms/step - loss: 0.1781 - acc: 0.9352 - val_loss: 0.9520 - val_acc: 0.7589\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 88/100\n","24/24 [==============================] - 1s 37ms/step - loss: 0.1315 - acc: 0.9568 - val_loss: 1.0051 - val_acc: 0.8000\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 89/100\n","24/24 [==============================] - 1s 37ms/step - loss: 0.1822 - acc: 0.9352 - val_loss: 0.7403 - val_acc: 0.8274\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 90/100\n","24/24 [==============================] - 1s 35ms/step - loss: 0.3222 - acc: 0.8907 - val_loss: 1.0566 - val_acc: 0.6959\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 91/100\n","24/24 [==============================] - 1s 35ms/step - loss: 0.3397 - acc: 0.9028 - val_loss: 1.3987 - val_acc: 0.5479\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 92/100\n","24/24 [==============================] - 1s 37ms/step - loss: 0.3829 - acc: 0.8745 - val_loss: 0.6881 - val_acc: 0.7890\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 93/100\n","24/24 [==============================] - 1s 38ms/step - loss: 0.1624 - acc: 0.9474 - val_loss: 1.3735 - val_acc: 0.6685\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 94/100\n","24/24 [==============================] - 1s 37ms/step - loss: 0.1681 - acc: 0.9379 - val_loss: 1.3813 - val_acc: 0.7096\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 95/100\n","24/24 [==============================] - 1s 36ms/step - loss: 0.2259 - acc: 0.9204 - val_loss: 0.8809 - val_acc: 0.7753\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 96/100\n","24/24 [==============================] - 1s 37ms/step - loss: 0.1216 - acc: 0.9528 - val_loss: 0.7260 - val_acc: 0.8521\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 97/100\n","24/24 [==============================] - 1s 37ms/step - loss: 0.1996 - acc: 0.9285 - val_loss: 0.5690 - val_acc: 0.8658\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 98/100\n","24/24 [==============================] - 1s 37ms/step - loss: 0.1394 - acc: 0.9420 - val_loss: 0.8840 - val_acc: 0.8055\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 99/100\n","24/24 [==============================] - 1s 36ms/step - loss: 0.0988 - acc: 0.9636 - val_loss: 0.7227 - val_acc: 0.8411\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 100/100\n","24/24 [==============================] - 1s 38ms/step - loss: 0.2176 - acc: 0.9217 - val_loss: 0.6944 - val_acc: 0.7945\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"]}],"source":["history = model.fit(X_train, y_train,\n","batch_size=batch_size,\n","epochs=100,\n","validation_data=(X_test, y_test),\n","verbose=1,\n","callbacks=[tensorboard, model_checkpoint_callback])"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1671563961016,"user":{"displayName":"victor cilleros","userId":"04822214906677236965"},"user_tz":-60},"id":"6Ne7VmUNTnzR"},"outputs":[],"source":["model.save_weights('model_DENSE_weights.h5')\n","model.save('model_DENSE.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Stly5G12vQrx"},"outputs":[],"source":["#On charge le modèle avec les meilleurs weights\n","model.load_weights(checkpoint_filepath)"]},{"cell_type":"markdown","metadata":{"id":"0t3DXacGOBud"},"source":["Now, in order to use the vector of these distances to classify emotions from \n","an image, you detect the face region, extract the facial landmarks and then compute the 4624 distances between them.\n","\n","The following script allow to capture the face region using Haar detector\n","(haarcascade_frontalface_default.xml) and detect 68 facial landmarks using the learned model (shape_predictor_68_face_landmarks.dat):"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":1928,"status":"ok","timestamp":1671563998937,"user":{"displayName":"victor cilleros","userId":"04822214906677236965"},"user_tz":-60},"id":"qAlprCjnN_uF"},"outputs":[],"source":["import cv2\n","from imutils import face_utils\n","import imutils\n","import dlib\n","# -----------\n","\n","face_cascade = cv2.CascadeClassifier('./Ressources/haarcascade_frontalface_default.xml')\n","# initialize dlib's face detector and create a predictor\n","detector = dlib.get_frontal_face_detector()\n","predictor = dlib.shape_predictor(\"./Ressources/shape_predictor_68_face_landmarks.dat\")"]},{"cell_type":"markdown","metadata":{"id":"TCLGnZ8W0jco"},"source":["# Detection via des images exemples"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":236},"executionInfo":{"elapsed":8,"status":"error","timestamp":1671563998937,"user":{"displayName":"victor cilleros","userId":"04822214906677236965"},"user_tz":-60},"id":"5bm1zUpWOHQ8","outputId":"0f838f76-5e7e-4ba6-8d37-83fbb2373469"},"outputs":[{"output_type":"error","ename":"error","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-85e87c05f78a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./example_image.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mgray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mfaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_cascade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetectMultiScale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31merror\u001b[0m: OpenCV(4.6.0) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"]}],"source":["import matplotlib.pyplot as plt\n","from google.colab.patches import cv2_imshow\n","\n","def detect_parts(image):\n","    # resize the image, and convert it to grayscale\n","    image = imutils.resize(image, width=200, height=200)\n","\n","    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","    # detect faces in the grayscale image\n","    rects = detector(gray, 1)\n","\n","    # loop over the face detections\n","    shape = None\n","    for (i, rect) in enumerate(rects):\n","        shape = predictor(gray, rect)\n","        shape = face_utils.shape_to_np(shape)\n","        \n","        # visualize all facial landmarks with a transparent overlay\n","        #output = face_utils.visualize_facial_landmarks(image, shape)\n","        #cv2.imshow(\"Image\", output)\n","        #cv2.waitKey(0)\n","    return shape\n","\n","img = cv2.imread(\"./example_image.png\")\n","gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n","\n","emotions = ('Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise')\n","for (x, y, w, h) in faces:\n","    cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)  # draw rectangle to main image\n","    detected_face = img[int(y):int(y + h), int(x):int(x + w)]  # crop detected face\n","    distances = euclidean_all_numpy(detect_parts(detected_face))\n","    predictions = model.predict(distances.reshape(1,-1))\n","    emotion = emotions[np.argmax(predictions)]\n","\n","    # write emotion text above rectangle\n","    cv2.putText(img, emotion, (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n","\n","cv2_imshow(img)\n","cv2.waitKey(0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L7bc9KyfwEw5"},"outputs":[],"source":["img = cv2.imread(\"./example_image2.jpg\")\n","gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n","\n","emotions = ('Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise')\n","for (x, y, w, h) in faces:\n","    cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)  # draw rectangle to main image\n","    detected_face = img[int(y):int(y + h), int(x):int(x + w)]  # crop detected face\n","    distances = euclidean_all_numpy(detect_parts(detected_face))\n","    predictions = model.predict(distances.reshape(1,-1))\n","    emotion = emotions[np.argmax(predictions)]\n","\n","    # write emotion text above rectangle\n","    cv2.putText(img, emotion, (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n","\n","cv2_imshow(img)\n","cv2.waitKey(0)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"executionInfo":{"elapsed":270,"status":"error","timestamp":1671330774735,"user":{"displayName":"victor cilleros","userId":"04822214906677236965"},"user_tz":-60},"id":"-15TqRViwJcB","outputId":"87685572-f6c8-4242-a649-e4bd97e4c543"},"outputs":[{"ename":"error","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","\u001b[0;32m<ipython-input-33-da7bb869d272>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./example_image3.webp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_cascade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetectMultiScale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0memotions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Angry'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Disgust'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Fear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Happy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Neutral'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Sad'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Surprise'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31merror\u001b[0m: OpenCV(4.6.0) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"]}],"source":["img = cv2.imread(\"./example_image3.webp\")\n","gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n","\n","emotions = ('Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise')\n","for (x, y, w, h) in faces:\n","    cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)  # draw rectangle to main image\n","    detected_face = img[int(y):int(y + h), int(x):int(x + w)]  # crop detected face\n","    distances = euclidean_all_numpy(detect_parts(detected_face))\n","    predictions = model.predict(distances.reshape(1,-1))\n","    emotion = emotions[np.argmax(predictions)]\n","\n","    # write emotion text above rectangle\n","    cv2.putText(img, emotion, (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n","\n","cv2_imshow(img)\n","cv2.waitKey(0)"]},{"cell_type":"markdown","metadata":{"id":"B2R3G-LIT17Y"},"source":["To classify emotions in real time capture, you need first to capture your video streaming from a webcam:"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":236},"executionInfo":{"elapsed":215,"status":"error","timestamp":1671564010806,"user":{"displayName":"victor cilleros","userId":"04822214906677236965"},"user_tz":-60},"id":"llzVs6c7T2Mg","outputId":"ee07817f-7827-4bdd-d524-891eeee0e66b"},"outputs":[{"output_type":"error","ename":"error","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-9659d29aba8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m  \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m  \u001b[0mgray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m  \u001b[0mfaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_cascade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetectMultiScale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m  \u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31merror\u001b[0m: OpenCV(4.6.0) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"]}],"source":["cap = cv2.VideoCapture(0)\n","while (True):\n"," ret, img = cap.read()\n"," gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n"," faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n"," ...\n"," ...\n"," ...\n","\n","# kill open cv things\n","cap.release()\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{"id":"yTGDossy2M6Q"},"source":["# Questions : "]},{"cell_type":"markdown","metadata":{"id":"D5UdhewaT6go"},"source":["**Question 1**: Develop your emotion recognition system to perform the recognition in real time.\n","\n","\n","**Question 2**: Propose another model based on a CNN extracted directly from the frame images.\n","To do this, uses the subsets (train_test_faceImages.zip extracted from FER2013 dataset."]},{"cell_type":"markdown","metadata":{"id":"1OZOfl35yiRA"},"source":["# Question 1"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":206,"status":"ok","timestamp":1671564013372,"user":{"displayName":"victor cilleros","userId":"04822214906677236965"},"user_tz":-60},"id":"lH-TsBSRT7Jg"},"outputs":[],"source":["# import dependencies\n","from IPython.display import display, Javascript, Image\n","from google.colab.output import eval_js\n","from base64 import b64decode, b64encode\n","import cv2\n","import numpy as np\n","import PIL\n","import io\n","import html\n","import time\n","from IPython.display import display, Javascript\n","from google.colab.output import eval_js\n","from base64 import b64decode"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1671564014362,"user":{"displayName":"victor cilleros","userId":"04822214906677236965"},"user_tz":-60},"id":"pm1ZlR_2ykUo"},"outputs":[],"source":["\n","def take_photo(filename='./photo.jpg', quality=0.8):\n","  js = Javascript('''\n","    async function takePhoto(quality) {\n","      const div = document.createElement('div');\n","      const capture = document.createElement('button');\n","      capture.textContent = 'Capture';\n","      div.appendChild(capture);\n","\n","      const video = document.createElement('video');\n","      video.style.display = 'block';\n","      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n","\n","      document.body.appendChild(div);\n","      div.appendChild(video);\n","      video.srcObject = stream;\n","      await video.play();\n","\n","      // Resize the output to fit the video element.\n","      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n","\n","      // Wait for Capture to be clicked.\n","      await new Promise((resolve) => capture.onclick = resolve);\n","\n","      const canvas = document.createElement('canvas');\n","      canvas.width = video.videoWidth;\n","      canvas.height = video.videoHeight;\n","      canvas.getContext('2d').drawImage(video, 0, 0);\n","      stream.getVideoTracks()[0].stop();\n","      div.remove();\n","      return canvas.toDataURL('image/jpeg', quality);\n","    }\n","    ''')\n","  display(js)\n","  data = eval_js('takePhoto({})'.format(quality))\n","  binary = b64decode(data.split(',')[1])\n","  with open(filename, 'wb') as f:\n","    f.write(binary)\n","  return filename"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nyD89IlIymco"},"outputs":[],"source":["while (True):\n","  filename = take_photo()\n","  img = cv2.imread(filename)\n","  gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","  faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n","  emotions = ('Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise')\n","  for (x, y, w, h) in faces:\n","      cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)  # draw rectangle to main image\n","      detected_face = img[int(y):int(y + h), int(x):int(x + w)]  # crop detected face\n","      distances = euclidean_all_numpy(detect_parts(detected_face))\n","      predictions = model.predict(distances.reshape(1,-1))\n","      emotion = emotions[np.argmax(predictions)]\n","\n","      # write emotion text above rectangle\n","      cv2.putText(img, emotion, (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n","      cv2_imshow(img)\n","# kill open cv things\n","cap.release()\n","cv2.destroyAllWindows()"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":228,"status":"ok","timestamp":1671564029942,"user":{"displayName":"victor cilleros","userId":"04822214906677236965"},"user_tz":-60},"id":"u-uISd90ypOI"},"outputs":[],"source":["# function to convert the JavaScript object into an OpenCV image\n","def js_to_image(js_reply):\n","  \"\"\"\n","  Params:\n","          js_reply: JavaScript object containing image from webcam\n","  Returns:\n","          img: OpenCV BGR image\n","  \"\"\"\n","  # decode base64 image\n","  image_bytes = b64decode(js_reply.split(',')[1])\n","  # convert bytes to numpy array\n","  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n","  # decode numpy array into OpenCV BGR image\n","  img = cv2.imdecode(jpg_as_np, flags=1)\n","\n","  return img\n","\n","# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n","def bbox_to_bytes(bbox_array):\n","  \"\"\"\n","  Params:\n","          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n","  Returns:\n","        bytes: Base64 image byte string\n","  \"\"\"\n","  # convert array into PIL image\n","  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n","  iobuf = io.BytesIO()\n","  # format bbox into png for return\n","  bbox_PIL.save(iobuf, format='png')\n","  # format return string\n","  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n","\n","  return bbox_bytes"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1671564032395,"user":{"displayName":"victor cilleros","userId":"04822214906677236965"},"user_tz":-60},"id":"pt11yRkryrI4"},"outputs":[],"source":["# JavaScript to properly create our live video stream using our webcam as input\n","def video_stream():\n","  js = Javascript('''\n","    var video;\n","    var div = null;\n","    var stream;\n","    var captureCanvas;\n","    var imgElement;\n","    var labelElement;\n","    \n","    var pendingResolve = null;\n","    var shutdown = false;\n","    \n","    function removeDom() {\n","       stream.getVideoTracks()[0].stop();\n","       video.remove();\n","       div.remove();\n","       video = null;\n","       div = null;\n","       stream = null;\n","       imgElement = null;\n","       captureCanvas = null;\n","       labelElement = null;\n","    }\n","    \n","    function onAnimationFrame() {\n","      if (!shutdown) {\n","        window.requestAnimationFrame(onAnimationFrame);\n","      }\n","      if (pendingResolve) {\n","        var result = \"\";\n","        if (!shutdown) {\n","          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n","          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n","        }\n","        var lp = pendingResolve;\n","        pendingResolve = null;\n","        lp(result);\n","      }\n","    }\n","    \n","    async function createDom() {\n","      if (div !== null) {\n","        return stream;\n","      }\n","\n","      div = document.createElement('div');\n","      div.style.border = '2px solid black';\n","      div.style.padding = '3px';\n","      div.style.width = '100%';\n","      div.style.maxWidth = '600px';\n","      document.body.appendChild(div);\n","      \n","      const modelOut = document.createElement('div');\n","      modelOut.innerHTML = \"<span>Status:</span>\";\n","      labelElement = document.createElement('span');\n","      labelElement.innerText = 'No data';\n","      labelElement.style.fontWeight = 'bold';\n","      modelOut.appendChild(labelElement);\n","      div.appendChild(modelOut);\n","           \n","      video = document.createElement('video');\n","      video.style.display = 'block';\n","      video.width = div.clientWidth - 6;\n","      video.setAttribute('playsinline', '');\n","      video.onclick = () => { shutdown = true; };\n","      stream = await navigator.mediaDevices.getUserMedia(\n","          {video: { facingMode: \"environment\"}});\n","      div.appendChild(video);\n","\n","      imgElement = document.createElement('img');\n","      imgElement.style.position = 'absolute';\n","      imgElement.style.zIndex = 1;\n","      imgElement.onclick = () => { shutdown = true; };\n","      div.appendChild(imgElement);\n","      \n","      const instruction = document.createElement('div');\n","      instruction.innerHTML = \n","          '<span style=\"color: red; font-weight: bold;\">' +\n","          'When finished, click here or on the video to stop this demo</span>';\n","      div.appendChild(instruction);\n","      instruction.onclick = () => { shutdown = true; };\n","      \n","      video.srcObject = stream;\n","      await video.play();\n","\n","      captureCanvas = document.createElement('canvas');\n","      captureCanvas.width = 640; //video.videoWidth;\n","      captureCanvas.height = 480; //video.videoHeight;\n","      window.requestAnimationFrame(onAnimationFrame);\n","      \n","      return stream;\n","    }\n","    async function stream_frame(label, imgData) {\n","      if (shutdown) {\n","        removeDom();\n","        shutdown = false;\n","        return '';\n","      }\n","\n","      var preCreate = Date.now();\n","      stream = await createDom();\n","      \n","      var preShow = Date.now();\n","      if (label != \"\") {\n","        labelElement.innerHTML = label;\n","      }\n","            \n","      if (imgData != \"\") {\n","        var videoRect = video.getClientRects()[0];\n","        imgElement.style.top = videoRect.top + \"px\";\n","        imgElement.style.left = videoRect.left + \"px\";\n","        imgElement.style.width = videoRect.width + \"px\";\n","        imgElement.style.height = videoRect.height + \"px\";\n","        imgElement.src = imgData;\n","      }\n","      \n","      var preCapture = Date.now();\n","      var result = await new Promise(function(resolve, reject) {\n","        pendingResolve = resolve;\n","      });\n","      shutdown = false;\n","      \n","      return {'create': preShow - preCreate, \n","              'show': preCapture - preShow, \n","              'capture': Date.now() - preCapture,\n","              'img': result};\n","    }\n","    ''')\n","\n","  display(js)\n","  \n","def video_frame(label, bbox):\n","  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n","  return data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_RUE96noysvw"},"outputs":[],"source":["# start streaming video from webcam\n","video_stream()\n","# label for video\n","label_html = 'Capturing...'\n","# initialze bounding box to empty\n","bbox = ''\n","count = 0 \n","emotions = ('Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise')\n","while True:\n","    js_reply = video_frame(label_html, bbox)\n","    if not js_reply:\n","        break\n","\n","    # convert JS response to OpenCV Image\n","    img = js_to_image(js_reply[\"img\"])\n","\n","    # create transparent overlay for bounding box\n","    bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n","\n","    # grayscale image for face detection\n","    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n","\n","    # get face region coordinates\n","    faces = face_cascade.detectMultiScale(gray)\n","    # get face bounding box for overlay\n","    for (x,y,w,h) in faces:\n","      bbox_array = cv2.rectangle(bbox_array,(x,y),(x+w,y+h),(255,0,0),2)\n","      detected_face = img[int(y):int(y + h), int(x):int(x + w)]  # crop detected face\n","      shape = detect_parts(detected_face)\n","      if shape is not None:\n","        distances = euclidean_all_numpy(shape)\n","        predictions = model.predict(distances.reshape(1,-1), verbose=0)\n","        emotion = emotions[np.argmax(predictions)]\n","\n","        bbox_array = cv2.putText(bbox_array, emotion, (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n","\n","    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n","    # convert overlay of bbox into bytes\n","    bbox_bytes = bbox_to_bytes(bbox_array)\n","    # update bbox so next frame gets new overlay\n","    bbox = bbox_bytes"]},{"cell_type":"markdown","metadata":{"id":"GB3QyHh7yvxw"},"source":["# Question 2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QQ8u8DXuyudI"},"outputs":[],"source":["X_train = np.load('./images_train.npy')\n","rgb_X_train = np.repeat(X_train.reshape(X_train.shape[:-1])[..., np.newaxis], 3, -1)\n","X_test = np.load('./images_test.npy')\n","rgb_X_test = np.repeat(X_test.reshape(X_test.shape[:-1])[..., np.newaxis], 3, -1)\n","y_train = np.load('./labels_train.npy')\n","y_test = np.load('./labels_test.npy')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qtzjtvIUyx1I"},"outputs":[],"source":["train_datagen = ImageDataGenerator(\n","rotation_range=40,\n","width_shift_range=0.2,\n","height_shift_range=0.2,\n","shear_range=0.2,\n","zoom_range=0.2,\n","horizontal_flip=True,)\n","val_datagen = ImageDataGenerator() #We do not augment validation data. we only perform rescale"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uiqoFMlpyzjJ"},"outputs":[],"source":["train_generator = train_datagen.flow(X_train, y_train,batch_size=batch_size)\n","val_generator = val_datagen.flow(X_test, y_test, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tqjy6FB5y0w4"},"outputs":[],"source":["CNN = models.Sequential() # (1)\n","CNN.add(layers.Conv2D(32, (3, 3), activation='relu',input_shape=X_train.shape[1:]))#(2)\n","CNN.add(layers.MaxPooling2D((2, 2)))\n","CNN.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","CNN.add(layers.MaxPooling2D((2, 2))) # (3)\n","CNN.add(layers.Conv2D(128, (3, 3), activation='relu'))\n","CNN.add(layers.MaxPooling2D((2, 2)))\n","CNN.add(layers.Conv2D(128, (3, 3), activation='relu'))\n","CNN.add(layers.MaxPooling2D((2, 2)))\n","CNN.add(layers.Flatten()) # (4)\n","CNN.add(layers.Dropout(0.5)) #Dropout for regularization # (5)\n","CNN.add(layers.Dense(512, activation='relu'))\n","CNN.add(layers.Dense(y_train.shape[1], activation='softmax'))\n","\n","CNN.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VgyoB_Xgy2BQ"},"outputs":[],"source":["CNN.compile(loss='categorical_crossentropy',optimizer=\"rmsprop\", metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ik49Z4rNy3XZ"},"outputs":[],"source":["NAME = \"CNN_TENSORBOARD\"\n","tensorboard = TensorBoard(log_dir=\"./logs/{}\".format(NAME))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4hp_eMQVy4sI"},"outputs":[],"source":["checkpoint_filepath = './tmp/CNN/checkpoint'\n","model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_filepath,\n","    save_weights_only=True,\n","    monitor='val_accuracy',\n","    mode='max',\n","    save_best_only=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ws-2rlply6Ko"},"outputs":[],"source":["history = CNN.fit(train_generator,\n","epochs=20,\n","validation_data=val_generator,\n","callbacks=[tensorboard, model_checkpoint_callback])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OtGWRldiy7cw"},"outputs":[],"source":["train_datagen = ImageDataGenerator(rescale=255,\n","rotation_range=40,\n","width_shift_range=0.2,\n","height_shift_range=0.2,\n","shear_range=0.2,\n","zoom_range=0.2,\n","horizontal_flip=True,)\n","val_datagen = ImageDataGenerator(rescale=255) #We do not augment validation data. we only perform rescale"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R5qE7S39y8t4"},"outputs":[],"source":["batch_size = 32\n","\n","train_generator = train_datagen.flow(rgb_X_train, y_train,batch_size=batch_size)\n","val_generator = val_datagen.flow(rgb_X_test, y_test, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UobBdJ68y-EY"},"outputs":[],"source":["i = tf.keras.layers.Input([None, None, 3], dtype = tf.float32)\n","x = tf.keras.layers.Lambda(lambda image: tf.image.resize(image, (224,224)))(i)\n","x = tf.keras.applications.vgg16.preprocess_input(x)\n","core = tf.keras.applications.vgg16.VGG16()\n","for l in core.layers:\n","  l.trainable = False\n","x = core(x)\n","# Stacking a new simple convolutional network on top of it\n","#x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n","#x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n","#x = layers.Flatten()(x)\n","x = layers.Dense(256, activation='relu')(x)\n","x = layers.Dense(y_train.shape[1], activation='softmax')(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"36DYbz_Ry_mQ"},"outputs":[],"source":["CNN_from_VGG = tf.keras.Model(inputs=i, outputs=x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CtEo31pczA8Q"},"outputs":[],"source":["CNN_from_VGG.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cApJafZAzCQg"},"outputs":[],"source":["CNN_from_VGG.compile(loss='categorical_crossentropy',optimizer=\"adam\", metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dXMVBj1MzDgS"},"outputs":[],"source":["NAME = \"CNN_from_VGG\"\n","tensorboard = TensorBoard(log_dir=\"./logs/{}\".format(NAME))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cBgVHimnzGDA"},"outputs":[],"source":["checkpoint_filepath = './tmp/CNN_from_VGG/checkpoint'\n","model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_filepath,\n","    save_weights_only=True,\n","    monitor='val_accuracy',\n","    mode='max',\n","    save_best_only=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k-0KdyKYzHWx"},"outputs":[],"source":["history = CNN_from_VGG.fit(train_generator,\n","epochs=20,\n","validation_data=val_generator,\n","callbacks=[tensorboard, model_checkpoint_callback])"]}],"metadata":{"colab":{"collapsed_sections":["TCLGnZ8W0jco","1OZOfl35yiRA"],"provenance":[],"authorship_tag":"ABX9TyO688NEZAOuK0PO/rKCX3Vw"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}